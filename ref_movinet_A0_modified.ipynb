{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_CVoPLkrx96"
   },
   "source": [
    "# Video classification using transfer learning \n",
    "\n",
    "The dataset consists of 1641* videos (1240 for train, 200 for val, and 201 for test).  The dataset is balanced for *reduced* vs. *not_reduced* left ventricular ejection fraction.\n",
    "\n",
    "#### Video Pre-Processing:\n",
    "\n",
    "Using ffmpeg in another notebook, videos are pre-processed as follows;:\n",
    "\n",
    "- convert to mp4\n",
    "- reduce frame per sec to obtain a 50-frame clip \n",
    "\n",
    "####  Setting Details:\n",
    "- frame_transform = v2.Compose([v2.Compose([v2.ToImage(),\n",
    "                                  v2.Resize((172,172)),\n",
    "                                  v2.Grayscale(num_output_channels = 3),\n",
    "                                  v2.ToDtype(torch.float32, scale=True),\n",
    "                                  v2.Normalize(mean=[0.43216, 0.394666, 0.37645],\n",
    "                                                     std=[0.22803, 0.22145, 0.216989])\n",
    "\n",
    "- Hyperparameters: common to the four studied models\n",
    "\n",
    "  *It was needed for this model to avoid a last incomplete batch and thus reduced our standard dataset accordingly and used a batch size of 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some parts of this code are based on the Python script:\n",
    "# https://github.com/pytorch/tutorials/blob/master/beginner_source/transfer_learning_tutorial.py\n",
    "# License: BSD\n",
    "# For the MoViNet model, the code is in part based on th python script:\n",
    "#https://github.com/Atze00/MoViNet-pytorch/blob/main/movinet_tutorial.ipynb\n",
    "# License: BSD: MIT\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd \n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.io import read_video\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.datasets.folder import make_dataset\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from movinets import MoViNet\n",
    "from movinets.config import _C\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "rng = np.random.default_rng(seed=42)\n",
    "# OpenMP: number of parallel threads.\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting of the main hyper-parameters of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.001               # Learning rate\n",
    "batch_size = 20            # Number of samples for each training step\n",
    "num_epochs = 10              # Number of training epochs\n",
    "gamma_lr_scheduler = 0.0001    # Learning rate reduction applied every 10 epochs.\n",
    "start_time = time.time()    # Start of the computation timer\n",
    "clip_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "C0OyilhWdzV8"
   },
   "outputs": [],
   "source": [
    "# for local laptop\n",
    "path_video_data = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir =  \"movienet_50f/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_classes(dir):\n",
    "    classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
    "    classes.sort()\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "def get_samples(root, extensions=(\".mp4\", \".avi\")):\n",
    "    _, class_to_idx = _find_classes(root)\n",
    "    return make_dataset(root, class_to_idx, extensions=extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['not_reduced', 'reduced'], {'not_reduced': 0, 'reduced': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = _find_classes(data_dir + \"train\")\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, root, epoch_size=None, frame_transform=None, video_transform=None, clip_len=16):\n",
    "        super(RandomDataset).__init__()\n",
    "\n",
    "        self.samples = get_samples(root)\n",
    "\n",
    "        # Allow for temporal jittering\n",
    "        if epoch_size is None:\n",
    "            epoch_size = len(self.samples)\n",
    "        self.epoch_size = epoch_size\n",
    "\n",
    "        self.clip_len = clip_len\n",
    "        self.frame_transform = frame_transform\n",
    "        self.video_transform = video_transform\n",
    "\n",
    "    def __iter__(self):\n",
    "        rng = np.random.default_rng(seed=42)\n",
    "        random_index_list = rng.choice(self.epoch_size, size= self.epoch_size,\n",
    "                                replace = False).tolist()  \n",
    "        \n",
    "        for i in range(self.epoch_size):\n",
    "            # Get random sample (seed = 42)\n",
    "            path, target = self.samples[random_index_list[i]]  \n",
    "            # Get video object\n",
    "            vid = torchvision.io.VideoReader(path, \"video\")\n",
    "            metadata = vid.get_metadata()\n",
    "            video_frames = []  # video frame buffer\n",
    "\n",
    "            # Seek and return frames\n",
    "            #max_seek is not used here:\n",
    "            #max_seek = metadata[\"video\"]['duration'][0]  - (self.clip_len / metadata[\"video\"]['fps'][0])\n",
    "            start = 0. # was: random.uniform(0., max_seek), now no more random, start at 0\n",
    "            for frame in itertools.islice(vid.seek(start), self.clip_len):\n",
    "                video_frames.append(self.frame_transform(frame['data']))                \n",
    "                current_pts = frame['pts']\n",
    "            # Stack it into a tensor\n",
    "            video = torch.stack(video_frames, 0)\n",
    "            if self.video_transform:\n",
    "                video = self.video_transform(video)\n",
    "            output = {\n",
    "                'path': path,\n",
    "                'video': video,\n",
    "                'target': target,\n",
    "                'start': start,\n",
    "                'end': current_pts}\n",
    "            yield output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_transform = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our videos are already with size (112, 112)\n",
    "frame_transform = v2.Compose([v2.ToImage(),\n",
    "                              v2.Resize((172,172)), # for MoViNet-pytorch in tutorial\n",
    "                              v2.Grayscale(num_output_channels = 3),\n",
    "                              v2.ToDtype(torch.float32, scale=True),\n",
    "                              v2.Normalize(mean=[0.43216, 0.394666, 0.37645],\n",
    "                                                     std=[0.22803, 0.22145, 0.216989]),\n",
    "                  \n",
    "])                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 1243, 'val': 214, 'test': 201}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes ={}\n",
    "for phase in [\"train\", \"val\",\"test\"]: \n",
    "    dataset_sizes[phase] = len(get_samples(data_dir+phase+\"/\") ) \n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dic = {}\n",
    "dataloaders = {}\n",
    "\n",
    "for phase in [\"train\", \"val\",\"test\"]: \n",
    "    dataset_dic[phase] = RandomDataset(data_dir+phase+\"/\", epoch_size=None,\n",
    "                                       frame_transform=frame_transform,\n",
    "                                       video_transform = video_transform,\n",
    "                                       clip_len = clip_len\n",
    "                                      )\n",
    "    # drop_last and shuffled added for MoViNet\n",
    "    dataloaders[phase] = DataLoader(dataset_dic[phase],\n",
    "                                    batch_size=batch_size,\n",
    "                                    #shuffle = True, # not allowed for IterableDataset\n",
    "                                    drop_last = True, # necessary for MoVieNet\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "fV1_WIAlrx-E"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = 10000.0  # Large arbitrary number\n",
    "    best_acc_train = 0.0\n",
    "    best_loss_train = 10000.0  # Large arbitrary number\n",
    "    print(\"Training started:\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                # Set model to training mode\n",
    "                model.train()\n",
    "            else:\n",
    "                # Set model to evaluate mode\n",
    "                model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            n_batches = dataset_sizes[phase] // batch_size\n",
    "            it = 0\n",
    "           \n",
    "            for batch in dataloaders[phase]:\n",
    "                since_batch = time.time()\n",
    "                #next_data = next(data_iter)\n",
    "                inputs = batch['video']\n",
    "                labels = batch['target']\n",
    "                batch_size_ = len(inputs)\n",
    "                \n",
    "                inputs = torch.permute(inputs, (0,2,1,3,4)) \n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Track/compute gradient and make an optimization step only when training\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Print iteration results\n",
    "                running_loss += loss.item() * batch_size_\n",
    "                batch_corrects = torch.sum(preds == labels.data).item()\n",
    "                running_corrects += batch_corrects\n",
    "                print(\n",
    "                    \"Phase: {} Epoch: {}/{} Iter: {}/{} Batch time: {:.4f}\".format(\n",
    "                        phase,\n",
    "                        epoch + 1,\n",
    "                        num_epochs,\n",
    "                        it + 1,\n",
    "                        n_batches + 1,\n",
    "                        time.time() - since_batch,\n",
    "                    ),\n",
    "                    end=\"\\r\",\n",
    "                    flush=True,\n",
    "                )\n",
    "                it += 1\n",
    "\n",
    "            # Print epoch results\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            print(\n",
    "                \"Phase: {} Epoch: {}/{} Loss: {:.4f} Acc: {:.4f}        \".format(\n",
    "                    \"train\" if phase == \"train\" else \"val\",\n",
    "                    epoch + 1,\n",
    "                    num_epochs,\n",
    "                    epoch_loss,\n",
    "                    epoch_acc,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Check if this is the best model wrt previous epochs\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == \"val\" and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "            if phase == \"train\" and epoch_acc > best_acc_train:\n",
    "                best_acc_train = epoch_acc\n",
    "            if phase == \"train\" and epoch_loss < best_loss_train:\n",
    "                best_loss_train = epoch_loss\n",
    "\n",
    "            # Update learning rate\n",
    "            if phase == \"train\":\n",
    "                scheduler.step()\n",
    "\n",
    "    # Print final results\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\n",
    "        \"Training completed in {:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60)\n",
    "    )\n",
    "    print(\"Best test loss: {:.4f} | Best test accuracy: {:.4f}\".format(best_loss, best_acc))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_phase(model, count_max=10000, phase = 'test'):\n",
    "    path_list = []\n",
    "    prob_list = []\n",
    "    pred_list = []\n",
    "    class_list_pred = []\n",
    "    class_list_label = []\n",
    "    label_list = []\n",
    "    counter = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for next_data in dataset_dic[phase]: \n",
    "            \n",
    "            path = next_data['path']\n",
    "            inputs = next_data['video']\n",
    "            label = next_data['target']\n",
    "            \n",
    "            inputs = inputs[None, :, :, :, :]\n",
    "\n",
    "            inputs = torch.permute(inputs, (0,2,1,3,4))      \n",
    "                   \n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = nn.Sigmoid()(outputs)\n",
    "            outputs = torch.nn.functional.normalize(outputs, p=1)\n",
    "            prob = outputs[0][1].item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            pred = int(preds[0].item())\n",
    "            class_pred = class_names[0][preds[0]]\n",
    "            class_label = class_names[0][label]         \n",
    "    \n",
    "            if counter%50 == 0:\n",
    "                print(counter)\n",
    "                \n",
    "            path_list.append(path)\n",
    "            class_list_pred.append(class_pred)\n",
    "            class_list_label.append(class_label)\n",
    "            prob_list.append(prob)                  \n",
    "            pred_list.append(pred)\n",
    "            label_list.append(label)\n",
    "            \n",
    "            counter += 1 \n",
    "            if counter == count_max: break\n",
    "    # create csv\n",
    "    df = pd.DataFrame(list(zip(path_list, class_list_label, class_list_pred, \n",
    "                               label_list, pred_list, prob_list)),\n",
    "               columns =['path', 'true_class', 'pred_class', 'label', 'pred', 'prob']) \n",
    "        \n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJwj-mfTN-zL"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "f7lBCVxIrdY7"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2804,
     "status": "ok",
     "timestamp": 1709309326532,
     "user": {
      "displayName": "Pierre Decoodt",
      "userId": "11089793044290653285"
     },
     "user_tz": -60
    },
    "id": "iu1jTJOhkybQ",
    "outputId": "8c6a49e6-a196-4287-e04a-e27d20c783d9"
   },
   "outputs": [],
   "source": [
    "model = MoViNet(_C.MODEL.MoViNetA0, causal = False, pretrained = True ) # CAUSAL IS FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "E9dVoEmPtYBb"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9dVoEmPtYBb"
   },
   "outputs": [],
   "source": [
    "model.classifier[3] = nn.Conv3d(2048, 2, kernel_size = (1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "E9dVoEmPtYBb"
   },
   "outputs": [],
   "source": [
    "# Use CUDA or CPU according to the \"device\" object.\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized\n",
    "optimizer = optim.Adam(model.classifier[3].parameters(), lr=step)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(\n",
    "    optimizer, step_size=10, gamma=gamma_lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "qHHTltHZfxm-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started:\n",
      "Phase: train Epoch: 1/10 Loss: 0.6329 Acc: 0.6597        \n",
      "Phase: val Epoch: 1/10 Loss: 0.5328 Acc: 0.7009        \n",
      "Phase: train Epoch: 2/10 Loss: 0.5585 Acc: 0.7273        \n",
      "Phase: val Epoch: 2/10 Loss: 0.4881 Acc: 0.7103        \n",
      "Phase: train Epoch: 3/10 Loss: 0.5233 Acc: 0.7498        \n",
      "Phase: val Epoch: 3/10 Loss: 0.4637 Acc: 0.7477        \n",
      "Phase: train Epoch: 4/10 Loss: 0.4997 Acc: 0.7659        \n",
      "Phase: val Epoch: 4/10 Loss: 0.4487 Acc: 0.7477        \n",
      "Phase: train Epoch: 5/10 Loss: 0.4828 Acc: 0.7747        \n",
      "Phase: val Epoch: 5/10 Loss: 0.4382 Acc: 0.7523        \n",
      "Phase: train Epoch: 6/10 Loss: 0.4672 Acc: 0.7788        \n",
      "Phase: val Epoch: 6/10 Loss: 0.4305 Acc: 0.7570        \n",
      "Phase: train Epoch: 7/10 Loss: 0.4594 Acc: 0.7844        \n",
      "Phase: val Epoch: 7/10 Loss: 0.4251 Acc: 0.7617        \n",
      "Phase: train Epoch: 8/10 Loss: 0.4503 Acc: 0.7981        \n",
      "Phase: val Epoch: 8/10 Loss: 0.4203 Acc: 0.7617        \n",
      "Phase: train Epoch: 9/10 Loss: 0.4471 Acc: 0.7908        \n",
      "Phase: val Epoch: 9/10 Loss: 0.4171 Acc: 0.7664        \n",
      "Phase: train Epoch: 10/10 Loss: 0.4366 Acc: 0.8069        \n",
      "Phase: val Epoch: 10/10 Loss: 0.4164 Acc: 0.7617        \n",
      "Training completed in 273m 35s\n",
      "Best test loss: 0.4164 | Best test accuracy: 0.7664\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = train_model(\n",
    "    model, criterion, optimizer, exp_lr_scheduler, num_epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pt6MSJit-rvR"
   },
   "source": [
    "## Predictions and accuracy for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\decpi\\Anaconda3\\envs\\video\\Lib\\site-packages\\torchvision\\io\\video_reader.py:233: UserWarning: Accurate seek is not implemented for pyav backend\n",
      "  warnings.warn(\"Accurate seek is not implemented for pyav backend\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "df = predict_for_phase(model, count_max = 20000, phase = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7761194029850746"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for i, label in enumerate(df['label']):\n",
    "    if label == df.pred[i]:\n",
    "        correct += 1\n",
    "accuracy = correct / len(df)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = \"redo_movinet_A0_modified.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(csv_name, index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
